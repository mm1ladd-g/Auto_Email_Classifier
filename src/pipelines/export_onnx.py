# src/pipelines/export_onnx.py
"""
Convert the fineâ€‘tuned MiniLM to ONNX (fp32) + INTâ€‘8.

Run:
    python src/pipelines/export_onnx.py
"""

from pathlib import Path
from onnxruntime.quantization import quantize_dynamic, QuantType
from optimum.onnxruntime import ORTModelForSequenceClassification
from transformers import AutoTokenizer

HF_DIR    = Path("models/minilm-epoch3")
OUT_DIR   = Path("models")
FP32_FILE = OUT_DIR / "model.onnx"
INT8_FILE = OUT_DIR / "minilm-int8.onnx"

def main():
    print("ðŸ”„  Exporting HF â†’ ONNX â€¦")
    ort_model = ORTModelForSequenceClassification.from_pretrained(
        HF_DIR,
        export=True,          # <-- current, stable flag
    )
    ort_model.save_pretrained(OUT_DIR)    # writes model.onnx

    print("âš™ï¸  Quantising INTâ€‘8 â€¦")
    quantize_dynamic(
        model_input=str(FP32_FILE),
        model_output=str(INT8_FILE),
        weight_type=QuantType.QInt8,
    )

    AutoTokenizer.from_pretrained(HF_DIR).save_pretrained(OUT_DIR)

    print("âœ”ï¸  fp32  â†’", FP32_FILE)
    print("âœ”ï¸  int8  â†’", INT8_FILE)

if __name__ == "__main__":
    main()
